# -*- coding: utf-8 -*-
"""optimization project 1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HLd4jc2USHpMunkJQvUkP94ZxDpHlZEL
"""

import numpy as np

def golden_section(g, a, b, tol=1e-14):#defining as stated in the instructions
    """This section is dedicated to Golden Section search on [a,b]."""
    rho = (3 - np.sqrt(5)) / 2  # this is just the natural coefficient for the golden ratio that looks like a p
    """defining the coefficients and their functions """
    a_1 = a + rho * (b - a)
    b_1 = a + (1 - rho) * (b - a)
    g_a, g_b = g(a_1), g(b_1)

    while (b - a) > tol: #the loop continues on as long as the interval length is greater than the tolerance, ensuring precision
        if g_a < g_b:
            b, b_1, g_b = b_1, a_1, g_a
            a_1 = a + rho * (b - a)
            g_a = g(a_1)
            # basically, if the condition is met, the right endpoint b is updated to
        # b_1, b_1 takes the previous position of a_1, and g_b takes g_a
        # position
        else:
            a, a_1, g_a = a_1, b_1, g_b
            b_1 = a + (1 - rho) * (b - a)
            g_b = g(b_1)
    #otherwise, left endpoint a is updated to a_1, a_1 becomes b_1, and g_a becomes g_b
    return (a + b) / 2

def coordinate_descent(f, x_0, b=1.0, search_tol=1e-14, conv_tol=1e-7, max_iterations=5000):
    """Coordinate Descent Algorithm."""
    x = np.array(x_0, dtype=float) #convert intial pt x_0 into numpy array
    n = len(x) #get the # of variables as a dimension of x
    d = np.zeros(n) #initialize zero vector for direction

    for k in range(max_iterations): #iterate up to max_its times
        x_old = np.copy(x) #store previous one to check for changes

        for j in range(n): #loop over each coordinate j
            d[j] = 1  # Select coordinate direction
            alpha_min = golden_section(lambda alpha: f(x + alpha * d), -b, b, search_tol)
            x += alpha_min * d #update x along coordinate j
            d[j] = 0  # Reset direction vector

        if np.linalg.norm(x - x_old) <= conv_tol:
            return x, k + 1  # Return the minimizer and iteration count

    return x, max_iterations  # If max iterations reached, return last x

#  TESTING GOLDEN SECTION SEARCH
f_1 = lambda x: x**2 + 4 * np.cos(x)
df_1 = lambda x: 2*x - 4*np.sin(x)
x1_star = golden_section(f_1, 1, 2, tol=1e-14)
df_star = df_1(x1_star)
print(f"Minimizer of f_1(x) = x^2 + 4cos(x): x* = {x1_star:.14f}")
print(f"Derivative at x*: df1(x*) = {df_star:.14f}")

# TESTING COORDINATE DESCENT ON f_2(x)
f_2 = lambda x: 12 - 8*x[2] + 2*x[0]**2 - 2*x[0]*x[1] + 2*x[1]**2 - 2*x[1]*x[2] + 2*x[2]**2
x2_analytic = np.array([2, 1, 2])  # Precomputed minimizer
x2_0 = np.array([-1, 2, -1])
x2_star, iter_f_2 = coordinate_descent(f2, x2_0, b=1.0, search_tol=1e-14, conv_tol=1e-7, max_iterations=5000)
print(f"Coordinate Descent on f_2(x): x* = {x2_star}, Iterations: {iter_f_2}")

# TESTING COORDINATE DESCENT ON ROSENBROCK FUNCTION
f3 = lambda x: (1 - x[0])**2 + 100 * (x[1] - x[0]**2)**2
x3_analytic = np.array([1, 1])  # Known minimizer
x3_0 = np.array([-1, -1.2])
x3_star, iter_f3 = coordinate_descent(f3, x3_0, b=1.0, search_tol=1e-14, conv_tol=1e-7, max_iterations=50000)




''' In this code I've analyzed Golden Section Search and coordinate descent to 3
different optimization problems

For f_1, the algorithm finds that x* is about 1.6481 and the derivative
is approximately 2.27 * 10^(-14), which is very small, and therefore verifying
the results. When larger tolerances are used, e.g 10^(-9) the results have fewer
iterations but the magnitude of the derivative is larger.

For f_2, the exact minimizer is found at (2,1,2) and it converges to such in
37 iterations. Also found through the program, the number of iterations
increases when the convergence tolerance decreases.

With f_1, it did not exactly converge within the maximum possible 5000 iterations,
but it approached (0.999,0.998). Because the Rosenbrock function is not linear,
there is more of a struggle to find a minimezer, at least when  using the method
of Coordinate Descent

To conclude, Coordinate Descent does well on quadratic functions but it requires
more iterations for accuracy. With Golden Section Search, it converges efficiently
but having iterations go beyond 10^(-10) doesn't really have much benefit. And lastly
Rosenbrock likely needs a different method to have more accuracy due to its
intricate nature
'''
print(f"Coordinate Descent on Rosenbrock function: x* = {x3_star}, Iterations: {iter_f3}")